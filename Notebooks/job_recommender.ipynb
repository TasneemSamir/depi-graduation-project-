{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEcWUaMDqh58",
        "outputId": "842a546f-3803-47bb-b8ae-620ed8766500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Enviroment"
      ],
      "metadata": {
        "id": "pDz42HTarftP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandarallel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKO4nWqnrnyW",
        "outputId": "bc00d678-b291-43f7-ece3-eda32df65046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandarallel\n",
            "  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from pandarallel) (0.3.8)\n",
            "Requirement already satisfied: pandas>=1 in /usr/local/lib/python3.12/dist-packages (from pandarallel) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from pandarallel) (5.9.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->pandarallel) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->pandarallel) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->pandarallel) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->pandarallel) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1->pandarallel) (1.17.0)\n",
            "Building wheels for collected packages: pandarallel\n",
            "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16674 sha256=f0316f9ac35f466439ac733382f001a1fcdf22b58973825b2b5b79dea70c453a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/f9/0d/40c9cd74a7cb8dc8fe57e8d6c3c19e2c730449c0d3f2bf66b5\n",
            "Successfully built pandarallel\n",
            "Installing collected packages: pandarallel\n",
            "Successfully installed pandarallel-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqkTQZQO8Y_i",
        "outputId": "6f55f1db-66fc-447c-942e-d816018d4526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Pandarallel will run on 1 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "id": "hcx4e5fWrfQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = '/content/drive/MyDrive/cv project/resume_classifier_model (3).keras'\n",
        "TOKENIZER_PATH = '/content/drive/MyDrive/cv project/tokenizer.pickle'\n",
        "ENCODER_PATH = '/content/drive/MyDrive/cv project/label_encoder (2).pickle'\n",
        "cv_lstm_functions='/content/drive/MyDrive/cv project/cv_lstm_functions.py'"
      ],
      "metadata": {
        "id": "JDo9omGSsYJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the directory of the script\n",
        "script_dir = os.path.dirname(cv_lstm_functions)\n",
        "# Add the directory to sys.path if not already present\n",
        "if script_dir not in sys.path:\n",
        "    sys.path.insert(0, script_dir)\n",
        "\n",
        "from cv_lstm_functions import *"
      ],
      "metadata": {
        "id": "P24g5UBfu3Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "asaniczka_1_3m_linkedin_jobs_and_skills_2024_path = kagglehub.dataset_download('asaniczka/1-3m-linkedin-jobs-and-skills-2024')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHPPSAV2rrLX",
        "outputId": "d8c4d376-bd7e-4884-f231-b6d3f47dd92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/asaniczka/1-3m-linkedin-jobs-and-skills-2024?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.88G/1.88G [00:22<00:00, 89.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "msbdeEpwrt3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(asaniczka_1_3m_linkedin_jobs_and_skills_2024_path + '/job_skills.csv')\n",
        "df2 = pd.read_csv(asaniczka_1_3m_linkedin_jobs_and_skills_2024_path + '/linkedin_job_postings.csv',usecols=['job_link', 'job_title', 'company','job_location'])"
      ],
      "metadata": {
        "id": "3K3A_gd5ruoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "    try:\n",
        "        model = load_model(MODEL_PATH)\n",
        "        with open(TOKENIZER_PATH, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "        with open(ENCODER_PATH, 'rb') as f:\n",
        "            encoder = pickle.load(f)\n",
        "\n",
        "        print(\"Models loaded successfully.\")\n",
        "        return model, tokenizer, encoder\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred loading models: {e}\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "DXuRw0vHvJ58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer, encoder = load_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU3CO0hrvxKf",
        "outputId": "87d21b45-ac6c-483a-ffe6-29cd6781d636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# resume preprocessing"
      ],
      "metadata": {
        "id": "QIx9Fh-p1n3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_resume(resume_text, verbose=True):\n",
        "  nlp = load_spacy_model()\n",
        "  cleaned_text = clean_resume_text(resume_text)\n",
        "\n",
        "  cleaned_df = pd.DataFrame({'Resume': [cleaned_text]})\n",
        "  processed_df = process_with_spacy(cleaned_df, nlp)  #lemmitization and POS\n",
        "  processed_text = processed_df['Resume_POS_text'].iloc[0]\n",
        "\n",
        "  return processed_text"
      ],
      "metadata": {
        "id": "P41GGKcwwX_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict category"
      ],
      "metadata": {
        "id": "qT73JV3n55np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_resume_category(resume_text,model=model,tokenizer=tokenizer,encoder=encoder,max_length=500):\n",
        "  #preprocess resume\n",
        "    processed_text = preprocess_resume(resume_text)\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "  #Pad sequences\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "    prediction_probs = model.predict(padded_sequence, verbose=0)[0]\n",
        "  #predicted class\n",
        "    predicted_index = np.argmax(prediction_probs)\n",
        "    predicted_category = encoder.inverse_transform([predicted_index])[0]\n",
        "\n",
        "  #confidence score\n",
        "    confidence = prediction_probs[predicted_index]\n",
        "\n",
        "    top_k = 5\n",
        "    top_indices = np.argsort(prediction_probs)[-top_k:][::-1]\n",
        "    top_categories = encoder.inverse_transform(top_indices)\n",
        "    top_probabilities = prediction_probs[top_indices]\n",
        "\n",
        "    return {\n",
        "        'predicted_category': predicted_category,\n",
        "        'confidence': float(confidence),\n",
        "        'top_5_predictions': [\n",
        "            {\n",
        "                'category': cat,\n",
        "                'probability': float(prob),\n",
        "                'percentage': f\"{prob * 100:.2f}%\"\n",
        "            }\n",
        "            for cat, prob in zip(top_categories, top_probabilities)\n",
        "        ]\n",
        "    }"
      ],
      "metadata": {
        "id": "ET_ISbar1nEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test category prediction"
      ],
      "metadata": {
        "id": "6ZVoHJAc6AIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_resume = \"\"\"SARAH JENKINS\n",
        "sarah.jenkins@email.com | (555) 987-6543 | linkedin.com/in/sarahjenkins\n",
        "\n",
        "SUMMARY\n",
        "Human Resources Generalist with 6 years of experience in employee relations,\n",
        "talent acquisition, and benefits administration. SHRM-CP certified.\n",
        "Passionate about creating positive and productive workplace cultures.\n",
        "\n",
        "SKILLS\n",
        "- Full-Cycle Recruiting\n",
        "- Onboarding & Training\n",
        "- Employee Relations\n",
        "- HRIS (Workday, BambooHR)\n",
        "- Benefits Administration\n",
        "- HR Policy & Compliance\n",
        "- Performance Management\n",
        "\n",
        "EXPERIENCE\n",
        "HR Generalist | Innovate Solutions | 2019-Present\n",
        "- Managed all aspects of the employee lifecycle for a 300-person tech company.\n",
        "- Led full-cycle recruiting, hiring over 50 new employees in 2023.\n",
        "- Administered employee benefits, payroll, and leave of absence programs.\n",
        "- Resolved complex employee relations issues and conducted investigations.\n",
        "\n",
        "HR Coordinator | DataCorp | 2017-2019\n",
        "- Assisted with new hire onboarding and orientation.\n",
        "- Maintained employee records and ensured HRIS data integrity.\n",
        "- Supported the HR team with compliance reporting and audits.\"\"\""
      ],
      "metadata": {
        "id": "g6pc7feW4eJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = predict_resume_category(\n",
        "    resume_text=sample_resume,\n",
        "    max_length=500)\n",
        "\n",
        "if result is not None:\n",
        "    print(f\"Predicted: {result['predicted_category']}\")\n",
        "    print(f\"Confidence: {result['confidence'] * 100:.2f}%\")\n",
        "else:\n",
        "    print(\"Prediction failed due to an error in loading model artifacts.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "ed8cb80977644b758461bcf9974a29e5",
            "91b5c9a9cdf146a781d498d651b28c1b",
            "41e466766519481fad336c55267ad6fd",
            "a2e8bec21aa54a588a8e26858fe8e777",
            "e04448d0e49344b4a6421d55f8fe2986",
            "0efd47ada93e410cbec146b6b91df968",
            "95617747d97e41dd9bfcb8a9db5bb783",
            "c27c9caf239c497d9169ba5303e99b47",
            "91104230e914465ca0cc3e1b1de8aec5",
            "163690ccffad40b8b0e7d9dd3dc0ea98",
            "b916276ce77f483793c7b4dbdde355e6"
          ]
        },
        "id": "ljiy2lRM37L7",
        "outputId": "6a95c16b-b7f9-45b1-9f20-9f60be97a019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy model 'en_core_web_sm' loaded successfully\n",
            "Processing text with spaCy (lemmatization & POS tagging)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Resumes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed8cb80977644b758461bcf9974a29e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy processing with POS tags completed.\n",
            "Predicted: human_resources\n",
            "Confidence: 53.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Job Recommendation based on predicted category"
      ],
      "metadata": {
        "id": "XiGuI95j6esr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Clean data"
      ],
      "metadata": {
        "id": "g64JqaTi7S0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jobs = df2.dropna(subset=['job_title', 'company'])\n",
        "jobs= jobs.drop_duplicates(subset=['job_link'])\n",
        "jobs = jobs.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(f\"Final dataset: {len(jobs):,} unique jobs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-L-YMBM4zBB",
        "outputId": "a61a8f0e-dbf2-4b7e-fef5-adef81b62c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset: 1,348,443 unique jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Match based on resume using job_title & job_skills\n",
        "def resume_job_similarity(resume_text, jobs_df, top_n=500):\n",
        "    print(f\"Resume-Job Similarity Matching\")\n",
        "\n",
        "    # Prepare job texts\n",
        "    jobs_df = jobs.copy()\n",
        "\n",
        "    # combines title and skills\n",
        "    if 'job_skills' in jobs_df.columns:\n",
        "        jobs_df['job_text'] = (\n",
        "            jobs_df['job_title'].fillna('') + ' ' +\n",
        "            jobs_df['job_skills'].fillna('')\n",
        "        )\n",
        "    else:\n",
        "        jobs_df['job_text'] = jobs_df['job_title'].fillna('')\n",
        "\n",
        "    jobs_df['job_text'] = jobs_df['job_text'].str.lower()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=1000,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2)\n",
        "    )\n",
        "\n",
        "    # Combine resume and job texts\n",
        "    all_texts = [resume_text.lower()] + jobs_df['job_text'].tolist()\n",
        "\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "    # Calculate similarity\n",
        "    resume_vector = tfidf_matrix[0:1]\n",
        "    job_vectors = tfidf_matrix[1:]\n",
        "    similarities = cosine_similarity(resume_vector, job_vectors)[0]\n",
        "    jobs_df['resume_score'] = similarities\n",
        "    result = jobs_df.nlargest(top_n, 'resume_score')\n",
        "\n",
        "    print(f\"Calculated resume similarity (top score: {result['resume_score'].max():.4f})\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "r2cagUdyMvcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Match jobs by category using TF-IDF on job_title\n",
        "def tfidf_job_matching(jobs_df, predicted_category, top_n=500):\n",
        "    category_text = predicted_category.replace('_', ' ')\n",
        "    jobs_df = jobs_df.copy()\n",
        "    jobs_df['job_text'] = jobs_df['job_title'].fillna('').str.lower()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english',\n",
        "                                  ngram_range=(1, 2), min_df=2)\n",
        "    all_texts = [category_text] + jobs_df['job_text'].tolist()\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "    category_vector = tfidf_matrix[0:1]\n",
        "    job_vectors = tfidf_matrix[1:]\n",
        "    similarities = cosine_similarity(category_vector, job_vectors)[0]\n",
        "\n",
        "    jobs_df['tfidf_score'] = similarities\n",
        "    return jobs_df.nlargest(top_n, 'tfidf_score')"
      ],
      "metadata": {
        "id": "2IedDMFr2fmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines both (60% category ,40% resume)\n",
        "def job_matching(jobs_df, predicted_category, resume_text=None, top_n=100):\n",
        "    print(f\"Job Matching: {predicted_category}\")\n",
        "\n",
        "    print(\"TF-IDF semantic matching\")\n",
        "    tfidf_matches = tfidf_job_matching(jobs_df, predicted_category, top_n=1000)\n",
        "    result = tfidf_matches.copy()\n",
        "\n",
        "    #Resume similarity (if provided)\n",
        "    if resume_text:\n",
        "        print(\"\\nResume-to-job similarity\")\n",
        "\n",
        "        resume_matches = resume_job_similarity(resume_text, result, top_n=len(result))\n",
        "\n",
        "        # Merge resume scores\n",
        "        result = result.merge(\n",
        "            resume_matches[['job_link', 'resume_score']],\n",
        "            on='job_link',\n",
        "            how='left'\n",
        "        )\n",
        "        result['resume_score'] = result['resume_score'].fillna(0)\n",
        "\n",
        "        # Normalize scores\n",
        "        if result['tfidf_score'].max() > 0:\n",
        "            result['tfidf_norm'] = result['tfidf_score'] / result['tfidf_score'].max()\n",
        "        else:\n",
        "            result['tfidf_norm'] = 0\n",
        "\n",
        "        if result['resume_score'].max() > 0:\n",
        "            result['resume_norm'] = result['resume_score'] / result['resume_score'].max()\n",
        "        else:\n",
        "            result['resume_norm'] = 0\n",
        "\n",
        "        result['final_score'] = (\n",
        "            0.60 * result['tfidf_norm'] +\n",
        "            0.40 * result['resume_norm']\n",
        "        )\n",
        "    else:\n",
        "        # Without resume: 100% TF-IDF score\n",
        "        print(\"\\nNo resume provided, skipping personalization.\")\n",
        "        if result['tfidf_score'].max() > 0:\n",
        "            result['tfidf_norm'] = result['tfidf_score'] / result['tfidf_score'].max()\n",
        "        else:\n",
        "            result['tfidf_norm'] = 0\n",
        "\n",
        "        # Calculate final score (100% TF-IDF)\n",
        "        result['final_score'] = result['tfidf_norm']\n",
        "\n",
        "    # Sort by final score\n",
        "    result = result.nlargest(top_n, 'final_score')\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"✓ FINAL: {len(result)} recommendations generated\")\n",
        "    print(f\"  Top score: {result['final_score'].max():.4f}\")\n",
        "    print(f\"  Average score: {result['final_score'].mean():.4f}\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "4X3Ql5Y8Na_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_recommendation_pipeline(resume_text,\n",
        "                                    predicted_category,\n",
        "                                    confidence,\n",
        "                                    jobs_df,\n",
        "                                    top_n=20):\n",
        "\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"Predicted Category: {predicted_category}\")\n",
        "    print(f\"Confidence: {confidence * 100:.2f}%\")\n",
        "    print(f\"{'#'*80}\\n\")\n",
        "\n",
        "    # Get recommendations\n",
        "    recommendations =job_matching(jobs_df,predicted_category, resume_text,top_n)\n",
        "    # Clean up columns for display\n",
        "    display_cols = ['job_link', 'job_title', 'company', 'job_location', 'final_score']\n",
        "    display_cols = [col for col in display_cols if col in recommendations.columns]\n",
        "\n",
        "    recommendations = recommendations[display_cols].copy()\n",
        "    recommendations.insert(0, 'rank', range(1, len(recommendations) + 1))\n",
        "    recommendations.rename(columns={'final_score': 'match_score'}, inplace=True)\n",
        "\n",
        "    return {\n",
        "        'predicted_category': predicted_category,\n",
        "        'confidence': confidence,\n",
        "        'method': 'automated (TF-IDF Title + Resume)',\n",
        "        'total_recommendations': len(recommendations),\n",
        "        'recommendations': recommendations\n",
        "    }"
      ],
      "metadata": {
        "id": "pRNfxISU7gWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_recommendations(result, show_top=10):\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"JOB RECOMMENDATIONS SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Predicted Role: {result['predicted_category']}\")\n",
        "    print(f\"Confidence: {result['confidence'] * 100:.2f}%\")\n",
        "    print(f\"Total Recommendations: {result['total_recommendations']}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    recommendations = result['recommendations']\n",
        "\n",
        "    print(f\"TOP {show_top} RECOMMENDED JOBS:\\n\")\n",
        "\n",
        "    for idx, row in recommendations.head(show_top).iterrows():\n",
        "        rank = row['rank']\n",
        "        title = row['job_title']\n",
        "        company = row['company']\n",
        "        location = row.get('job_location', 'N/A')\n",
        "        score = row.get('match_score', 0)\n",
        "\n",
        "        print(f\"{rank}. {title}\")\n",
        "        print(f\"   Company: {company}\")\n",
        "        print(f\"   Location: {location}\")\n",
        "        print(f\"   Match Score: {score:.4f}\")\n",
        "        print(f\"   Link: {row['job_link'][:80]}...\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "GtKuXF1VUp0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_category = result['predicted_category']\n",
        "confidence = result['confidence']\n",
        "result = complete_recommendation_pipeline(\n",
        "    resume_text= sample_resume,\n",
        "    predicted_category=predicted_category,\n",
        "    confidence=confidence,\n",
        "    jobs_df=jobs,\n",
        "    top_n=20\n",
        ")\n",
        "\n",
        "display_recommendations(result, show_top=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q9iq9A2Nw4w",
        "outputId": "57141833-b952-4709-c199-6d487d792b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################################################################\n",
            "Predicted Category: human_resources\n",
            "Confidence: 53.39%\n",
            "################################################################################\n",
            "\n",
            "Job Matching: human_resources\n",
            "TF-IDF semantic matching\n",
            "\n",
            "Resume-to-job similarity\n",
            "Resume-Job Similarity Matching\n",
            "Calculated resume similarity (top score: 0.7443)\n",
            "\n",
            "================================================================================\n",
            "✓ FINAL: 20 recommendations generated\n",
            "  Top score: 0.6000\n",
            "  Average score: 0.5889\n",
            "\n",
            "================================================================================\n",
            "JOB RECOMMENDATIONS SUMMARY\n",
            "================================================================================\n",
            "Predicted Role: human_resources\n",
            "Confidence: 53.39%\n",
            "Total Recommendations: 20\n",
            "================================================================================\n",
            "\n",
            "TOP 10 RECOMMENDED JOBS:\n",
            "\n",
            "1. Human Resources BP\n",
            "   Company: Schindler Group\n",
            "   Location: Atlanta, GA\n",
            "   Match Score: 0.6000\n",
            "   Link: https://www.linkedin.com/jobs/view/human-resources-bp-at-schindler-group-3745134...\n",
            "\n",
            "2. Human Resources BP\n",
            "   Company: Schindler Elevator Corporation (U.S.)\n",
            "   Location: Atlanta, GA\n",
            "   Match Score: 0.6000\n",
            "   Link: https://www.linkedin.com/jobs/view/human-resources-bp-at-schindler-elevator-corp...\n",
            "\n",
            "3. Human Resources Represenative\n",
            "   Company: Titan America\n",
            "   Location: Roanoke, VA\n",
            "   Match Score: 0.6000\n",
            "   Link: https://www.linkedin.com/jobs/view/human-resources-represenative-at-titan-americ...\n",
            "\n",
            "4. Human Resources Generalist_L1J15019\n",
            "   Company: Wilbert Plastic Services\n",
            "   Location: Québec, Quebec, Canada\n",
            "   Match Score: 0.6000\n",
            "   Link: https://ca.linkedin.com/jobs/view/human-resources-generalist-l1j15019-at-wilbert...\n",
            "\n",
            "5. Human Resources - Expressions of Interests\n",
            "   Company: Perigon Group\n",
            "   Location: Sydney, New South Wales, Australia\n",
            "   Match Score: 0.6000\n",
            "   Link: https://au.linkedin.com/jobs/view/human-resources-expressions-of-interests-at-pe...\n",
            "\n",
            "6. MANAGER - HUMAN RESOURCES\n",
            "   Company: Hard Rock Hotel & Casino Bristol\n",
            "   Location: Bristol, TN\n",
            "   Match Score: 0.5852\n",
            "   Link: https://www.linkedin.com/jobs/view/manager-human-resources-at-hard-rock-hotel-ca...\n",
            "\n",
            "7. Manager, Human Resources\n",
            "   Company: Long Beach Transit\n",
            "   Location: Long Beach, CA\n",
            "   Match Score: 0.5852\n",
            "   Link: https://www.linkedin.com/jobs/view/manager-human-resources-at-long-beach-transit...\n",
            "\n",
            "8. Manager-Human Resources\n",
            "   Company: Microchip Technology Inc.\n",
            "   Location: Beverly, MA\n",
            "   Match Score: 0.5852\n",
            "   Link: https://www.linkedin.com/jobs/view/manager-human-resources-at-microchip-technolo...\n",
            "\n",
            "9. Manager of Human Resources\n",
            "   Company: Branches, Inc.\n",
            "   Location: Miami, FL\n",
            "   Match Score: 0.5852\n",
            "   Link: https://www.linkedin.com/jobs/view/manager-of-human-resources-at-branches-inc-38...\n",
            "\n",
            "10. Manager, Human Resources\n",
            "   Company: W. R. Grace\n",
            "   Location: Baltimore, MD\n",
            "   Match Score: 0.5852\n",
            "   Link: https://www.linkedin.com/jobs/view/manager-human-resources-at-w-r-grace-37341837...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}