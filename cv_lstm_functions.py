# -*- coding: utf-8 -*-
"""CV LSTM Functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I-RUFKmgSZVNTpQdRuEM8VwvPnWa_6N9
"""

import pandas as pd
import numpy as np
import re
import pickle
import warnings
from pathlib import Path
import pytesseract
from pdf2image import convert_from_path, convert_from_bytes
from PIL import Image
import io

import spacy
from pandarallel import pandarallel
from tqdm.auto import tqdm

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandarallel

def setup_environment():
    """Initialize pandarallel and download required models"""
    pandarallel.initialize(progress_bar=True)
    print("Environment setup completed!")

def load_spacy_model(model_name='en_core_web_sm'):
    """Load spaCy model for NLP processing"""
    try:
        nlp = spacy.load(model_name)
        print(f"spaCy model '{model_name}' loaded successfully")
        return nlp
    except OSError:
        print(f"Downloading spaCy model '{model_name}'...")
        import os
        os.system(f"python -m spacy download {model_name}")
        nlp = spacy.load(model_name)
        return nlp

def load_kaggle_datasets(resume_dataset_path, cvscsv_path, curriculum_vitae_path, resume_analysis_path):
    """Load all Kaggle datasets and return list of dataframes"""
    df1 = pd.read_csv(f'{resume_dataset_path}/UpdatedResumeDataSet.csv')
    df2 = pd.read_csv(f'{cvscsv_path}/cvs.csv')
    df3 = pd.read_csv(f'{curriculum_vitae_path}/Curriculum Vitae.csv')
    df4 = pd.read_csv(f'{cvscsv_path}/cvs.csv')

    df5 = pd.read_csv(f'{resume_analysis_path}/Jillani SofTech Updated Resume Dataset.csv')
    df6 = pd.read_csv(f'{resume_analysis_path}/Jithin Jagadeesh gpt_dataset.csv')
    df7 = pd.read_csv(f'{resume_analysis_path}/Noor Saeed clean_resume_data.csv')
    df8 = pd.read_csv(f'{resume_analysis_path}/Snehaan Bhawal Resume Dataset.csv')
    df9 = pd.read_csv(f'{resume_analysis_path}/Wahib Mzali Resume data.csv')

    print(f"Loaded 9 datasets from Kaggle")
    return [df1, df2, df3, df4, df5, df6, df7, df8, df9]

def load_huggingface_datasets():
    """Load all HuggingFace datasets and return list of dataframes"""
    df10 = pd.read_csv("hf://datasets/AzharAli05/Resume-Screening-Dataset/dataset.csv")
    df10 = df10.drop(['Decision', 'Reason_for_decision', 'Job_Description'], axis=1)

    df11 = pd.read_csv("hf://datasets/ahmedheakl/resume-atlas/train.csv")

    df12 = pd.read_csv("hf://datasets/0xnbk/resume-domain-classifier-v1-en/validation.csv")
    df12 = df12.drop(['label', 'pair_type', 'job_domain'], axis=1)

    print(f"Loaded 3 datasets from HuggingFace")
    return [df10, df11, df12]

def standardize_column_names(dataframes):
    """Standardize column names across all dataframes"""
    df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12 = dataframes

    # Standardize df7
    df7 = df7.drop('ID', axis=1, errors='ignore')
    df7 = df7.rename(columns={'Feature': 'Resume'}, errors='ignore')

    # Standardize df8
    df8 = df8.drop(['ID', 'Resume_html'], axis=1, errors='ignore')
    df8 = df8.rename(columns={'Resume_str': 'Resume'}, errors='ignore')

    # Standardize df9
    df9 = df9.rename(columns={'Label': 'Category'}, errors='ignore')

    # Standardize df10, df11, df12
    df10 = df10.rename(columns={'Role': 'Category'})
    df11 = df11.rename(columns={'Text': 'Resume'})
    df12 = df12.rename(columns={'text': 'Resume', 'resume_domain': 'Category'})

    print("Column names standardized")
    return [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12]

def merge_datasets(dataframes):
    """Merge all datasets and remove duplicates"""
    df = pd.concat(dataframes, ignore_index=True)

    df.drop_duplicates(inplace=True)
    df.dropna(inplace=True)
    df.reset_index(drop=True, inplace=True)

    print(f"Combined dataset shape: {df.shape}")
    print(f"Unique categories: {df['Category'].nunique()}")

    return df

def clean_resume_text(text):
    """Clean resume text by removing URLs, emails, special characters"""
    if not isinstance(text, str):
        return ""

    # Remove URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)

    # Remove emails
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)

    # Remove non-alphabetic characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text.lower()

def apply_text_cleaning(df):
    """Apply text cleaning to Resume column"""
    print("Cleaning resume text...")
    df['Resume'] = df['Resume'].parallel_apply(clean_resume_text)
    print("Text cleaning completed")
    return df

def process_with_spacy(df, nlp):
    """Process text with spaCy for lemmatization and POS tagging"""
    print("Processing text with spaCy (lemmatization & POS tagging)...")

    texts = df['Resume'].tolist()
    processed_texts = []

    docs = nlp.pipe(texts, disable=["parser", "ner"], batch_size=50, n_process=-1)

    for doc in tqdm(docs, total=len(texts), desc="Processing Resumes"):
        processed_texts.append(' '.join([
            f"{token.lemma_}_{token.pos_}" for token in doc
            if not token.is_stop and not token.is_punct and not token.is_space
        ]))

    df['Resume_POS_text'] = processed_texts

    print("spaCy processing with POS tags completed.")
    return df

def normalize_job_titles(df):
    """Normalize job title variations to standard categories"""
    role_variations = {
        'software_developer': ['software engineer', 'software dev', 'programmer',
                              'application developer', 'software engineering', 'technology'],
        'java_developer': ['java engineer', 'java programmer'],
        'python_developer': ['python engineer', 'python programmer'],
        'web_developer': ['web dev', 'website developer'],
        'frontend_developer': ['front end', 'front-end', 'ui developer'],
        'backend_developer': ['back end', 'back-end'],
        'fullstack_developer': ['full stack', 'full-stack'],
        'data_scientist': ['data science'],
        'data_analyst': ['data analysis'],
        'data_engineer': ['data engineering'],
        'network_administrator': ['network admin', 'network engineer'],
        'system_administrator': ['sysadmin', 'systems administrator'],
        'devops_engineer': ['devops', 'dev ops'],
        'project_manager': ['program manager', 'technical project manager'],
        'qa_engineer': ['quality assurance', 'test engineer', 'tester'],
        'mobile_developer': ['android developer', 'ios developer'],
        'database_administrator': ['dba', 'database engineer'],
        'security_engineer': ['cybersecurity', 'information security'],
    }

    title_map = {}
    for base, variations in role_variations.items():
        title_map[base.replace('_', ' ')] = base
        for var in variations:
            title_map[var] = base

    def normalize_category(cat):
        if not isinstance(cat, str):
            return cat

        cat = cat.lower().strip()
        cat = re.sub(r'[_\-/]', ' ', cat)

        if cat in title_map:
            return title_map[cat]

        for key, value in title_map.items():
            if key in cat:
                return value

        return re.sub(r'\s+', '_', re.sub(r'[^a-z0-9\s]', '', cat).strip())

    df['Category'] = df['Category'].parallel_apply(normalize_category)
    return df

def consolidate_multi_labels(df):
    """Consolidate multiple labels into single best label"""
    priority_roles = [
        'java_developer', 'python_developer', 'web_developer',
        'frontend_developer', 'backend_developer', 'fullstack_developer',
        'mobile_developer', 'data_scientist', 'data_engineer', 'data_analyst',
        'devops_engineer', 'qa_engineer', 'security_engineer',
        'database_administrator', 'network_administrator', 'system_administrator',
        'project_manager', 'software_developer'
    ]
    def pick_best_label(cat):
        if not isinstance(cat, str) or ',' not in cat:
            return cat

        labels = [l.strip() for l in cat.split(',')]

        for priority_role in priority_roles:
            if priority_role in labels:
                return priority_role

        return labels[0]

    df['Category'] = df['Category'].parallel_apply(pick_best_label)
    return df

def clean_categories(df, min_samples=10):
    """Remove categories with fewer than min_samples"""
    print("\nCategory Distribution (Before Cleaning):")
    print(df['Category'].value_counts().head(20))

    counts = df['Category'].value_counts()
    valid = counts[counts >= min_samples].index
    df_filtered = df[df['Category'].isin(valid)]

    removed = len(df) - len(df_filtered)
    print(f"\nRemoved {removed} samples with rare categories")
    print(f"Final shape: {df_filtered.shape}")
    print(f"Unique categories: {df_filtered['Category'].nunique()}")

    return df_filtered

def apply_category_normalization(df, min_samples=10):
    """Apply all category normalization steps"""
    print("Normalizing categories...")
    df = normalize_job_titles(df)
    df = consolidate_multi_labels(df)
    df = clean_categories(df, min_samples=min_samples)
    print("Category normalization completed")
    return df

def create_tokenizer(texts, vocab_size=10000):
    """Create and fit tokenizer on texts"""
    tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
    tokenizer.fit_on_texts(texts)
    return tokenizer

def texts_to_sequences(tokenizer, texts, max_length=500):
    """Convert texts to padded sequences"""
    sequences = tokenizer.texts_to_sequences(texts)
    X_padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')

    print(f"Shape of padded sequences: {X_padded.shape}")
    return X_padded

def prepare_features(df, vocab_size=10000, max_length=500):
    """Prepare features from preprocessed text"""
    tokenizer = create_tokenizer(df['Resume_POS_text'], vocab_size=vocab_size)
    X_padded = texts_to_sequences(tokenizer, df['Resume_POS_text'], max_length=max_length)
    y = df['Category']

    return X_padded, y, tokenizer

def encode_labels(y):
    """Encode labels to categorical format"""
    encoder = LabelEncoder()
    y_encoded = encoder.fit_transform(y)
    y_categorical = to_categorical(y_encoded)

    return y_categorical, encoder

def split_data(X, y, test_size=0.2, random_state=42):
    """Split data into training and testing sets"""
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y
    )

    print(f"Data split into training and testing sets.")
    print(f"X_train shape: {X_train.shape}")
    print(f"X_test shape: {X_test.shape}")
    print(f"Number of categories: {y_train.shape[1]}")

    return X_train, X_test, y_train, y_test

def build_lstm_model(vocab_size, embedding_dim, max_length, num_classes, lstm_units=128, dropout=0.2):
    """Build LSTM model for resume classification"""
    model = Sequential()

    model.add(Embedding(input_dim=vocab_size,
                        output_dim=embedding_dim,
                        input_length=max_length))

    model.add(LSTM(lstm_units, dropout=dropout))

    model.add(Dense(num_classes, activation='softmax'))

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    model.build(input_shape=(None, max_length))

    print("Model built successfully!")
    model.summary()

    return model

def train_model(model, X_train, y_train, X_test, y_test, epochs=20, batch_size=64, patience=2):
    """Train the LSTM model"""
    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)

    print("Starting model training...")

    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_test, y_test),
        verbose=1,
        callbacks=[early_stopping]
    )

    print("Model training completed!")

    return history

def evaluate_model(model, X_test, y_test):
    """Evaluate model performance"""
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)

    print(f"Test Accuracy: {accuracy * 100:.2f}%")
    print(f"Test Loss: {loss:.4f}")

    return loss, accuracy

def generate_predictions(model, X_test):
    """Generate predictions from model"""
    y_pred_probs = model.predict(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)

    return y_pred, y_pred_probs

def print_classification_report(y_true, y_pred, encoder):
    """Print classification report"""
    print("Classification Report:")
    print(classification_report(y_true, y_pred, target_names=encoder.classes_))

def plot_confusion_matrix(y_true, y_pred, encoder, figsize=(15, 12)):
    """Plot confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=figsize)
    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',
                xticklabels=encoder.classes_, yticklabels=encoder.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()

def plot_training_history(history):
    """Plot training history"""
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')

    plt.tight_layout()
    plt.show()

def save_preprocessed_data(df, filepath='preprocessed_resumes.csv'):
    """Save preprocessed dataframe to CSV"""
    df.to_csv(filepath, index=False)
    print(f"Preprocessed data saved to {filepath}")

def load_preprocessed_data(filepath='preprocessed_resumes.csv'):
    """Load preprocessed dataframe from CSV"""
    df = pd.read_csv(filepath)
    print(f"Loaded preprocessed data from {filepath}")
    return df

def save_model_artifacts(model, tokenizer, encoder,
                         model_path='resume_classifier_model.keras',
                         tokenizer_path='tokenizer.pickle',
                         encoder_path='label_encoder.pickle'):
    """Save model, tokenizer, and encoder"""
    # Save model
    model.save(model_path)
    print(f"Model saved to {model_path}")

    with open(tokenizer_path, 'wb') as handle:
        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print(f"Tokenizer saved to {tokenizer_path}")

    with open(encoder_path, 'wb') as handle:
        pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print(f"Label encoder saved to {encoder_path}")

def load_model_artifacts(model_path='resume_classifier_model.keras',
                        tokenizer_path='tokenizer.pickle',
                        encoder_path='label_encoder.pickle'):
    """Load model, tokenizer, and encoder"""
    # Load model
    model = load_model(model_path)
    print(f"Model loaded from {model_path}")

    # Load tokenizer
    with open(tokenizer_path, 'rb') as handle:
        tokenizer = pickle.load(handle)
    print(f"Tokenizer loaded from {tokenizer_path}")

    # Load encoder
    with open(encoder_path, 'rb') as handle:
        encoder = pickle.load(handle)
    print(f"Label encoder loaded from {encoder_path}")

    return model, tokenizer, encoder

def predict_resume_category(resume_text, model, tokenizer, encoder, max_length=500):
    """
    تاخد نص الـ CV وترجّع:
      - category: اسم الكاتيجوري (string)
      - confidence: أعلى probability
    """
    # 1) preprocess + tokenize (نفس اللي كنت عامله في النوتبوك)
    seq = tokenizer.texts_to_sequences([resume_text])
    # padding حسب الـ max_length
    padded = pad_sequences(
        seq,
        maxlen=max_length,
        padding="post",
        truncating="post"
    )

    # 2) model prediction
    probs = model.predict(padded)[0]   # shape: (num_classes,)

    # 3) class index
    predicted_class = int(np.argmax(probs))
    confidence = float(np.max(probs))

    # 4) محاولة فكّ الكلاس بالـ encoder بشكل آمن
    try:
        # ده الطبيعي لو الـ encoder متوافق مع الموديل
        category = encoder.inverse_transform([predicted_class])[0]
    except ValueError:
        # هنا بنقع لو الموديل بيطلع index أكبر من عدد الكلاسات اللي جوه الـ encoder
        # نحاول نرجّع اسم من classes_ لو الـ index جوه الرينج
        if hasattr(encoder, "classes_") and predicted_class < len(encoder.classes_):
            category = encoder.classes_[predicted_class]
        else:
            # لو برضه مش نافع، نرجع اسم generic
            category = f"Class #{predicted_class}"

    return category, confidence

def extract_text_from_pdf_ocr(pdf_bytes, poppler_path=None):
    """
    Extract text from PDF bytes using OCR (Tesseract).
    Converts PDF pages to images first.
    """
    try:
        # Convert PDF to images
        images = convert_from_bytes(pdf_bytes, poppler_path=poppler_path)
        
        full_text = ""
        for i, image in enumerate(images):
            # Extract text from image
            text = pytesseract.image_to_string(image)
            full_text += f"\n--- Page {i+1} ---\n{text}"
            
        return full_text.strip()
    except Exception as e:
        print(f"Error in OCR extraction: {e}")
        return ""

def load_job_model(model_path='models/job_recommender.h5'):
    """Load the job recommender model"""
    try:
        model = load_model(model_path)
        print(f"Job Recommender Model loaded from {model_path}")
        return model
    except Exception as e:
        print(f"Failed to load job model: {e}")
        return None

def predict_job_role(resume_text, model, tokenizer, encoder, max_length=500, top_n=3):
    """
    Predict job role from resume text.
    Returns a list of top predictions to ensure dynamic output.
    """
    if model is None:
        return [{"role": "Model not loaded", "confidence": 0.0}]

    # 1) Preprocess
    seq = tokenizer.texts_to_sequences([resume_text])
    padded = pad_sequences(seq, maxlen=max_length, padding='post', truncating='post')

    # 2) Predict
    probs = model.predict(padded)[0]
    
    # 3) Get top N predictions
    top_indices = probs.argsort()[-top_n:][::-1]
    
    predictions = []
    for idx in top_indices:
        conf = float(probs[idx])
        try:
            if hasattr(encoder, "classes_") and idx < len(encoder.classes_):
                role = encoder.classes_[idx]
            else:
                role = encoder.inverse_transform([idx])[0]
        except:
            role = f"Class #{idx}"
            
        predictions.append({"role": role, "confidence": conf})
        
    return predictions

def plot_category_distribution(df, top_n=20, figsize=(14, 6)):
    """Plot category distribution"""
    plt.figure(figsize=figsize)
    df['Category'].value_counts().head(top_n).plot(kind='bar')
    plt.title(f'Top {top_n} Resume Categories Distribution')
    plt.xlabel('Category')
    plt.ylabel('Count')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

def print_dataset_info(df):
    """Print dataset information"""
    print("Dataset Information:")
    print(f"Total resumes: {len(df)}")
    print(f"Unique categories: {df['Category'].nunique()}")
    print(f"\nColumn types:")
    print(df.dtypes)
    print("\nTop 20 Categories:")
    print(df['Category'].value_counts().head(20))